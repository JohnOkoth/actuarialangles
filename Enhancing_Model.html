<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Dynamic Pricing With Reinforcement Learing">
  <title>Dynamic Pricing Optimization</title>

  <!-- Google Analytics Script -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9S5SM84Q3T"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9S5SM84Q3T');
  </script>

  <style>
    /* Reset and base styles */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }

    /* Sidebar */
    #side-menu {
      width: 260px;
      position: fixed;
      top: 60px;
      height: calc(100vh - 60px);
      background: #fff;
      border-right: 1px solid #e0e0e0;
      padding: 20px;
      transform: translateX(-100%);
      transition: transform 0.3s ease-in-out;
      box-shadow: 2px 0 5px rgba(0,0,0,0.05);
      z-index: 100;
    }
    #side-menu.open {
      transform: translateX(0);
    }
    #side-menu a {
      display: block;
      padding: 10px 15px;
      color: #555;
      text-decoration: none;
      font-weight: 500;
      margin-bottom: 10px;
      border-radius: 5px;
      transition: background 0.3s, color 0.3s;
    }
    #side-menu a:hover {
      background: #007BFF;
      color: #fff;
    }

    /* Menu Button */
    #menu-toggle {
      position: fixed;
      top: 15px;
      left: 15px;
      background: #007BFF;
      color: #fff;
      border: none;
      padding: 10px 20px;
      cursor: pointer;
      border-radius: 5px;
      z-index: 200;
      font-size: 16px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.2);
      transition: background 0.3s;
    }
    #menu-toggle:hover {
      background: #0056b3;
    }

    /* Main Content */
    #main-content {
      margin-left: 280px;
      padding: 40px 30px 60px 30px;
      max-width: 1200px;
    }
    header h1 {
      font-size: 2.2rem;
      margin-bottom: 20px;
      color: #007BFF;
      font-weight: bold;
    }
    section {
      margin-bottom: 60px;
    }
    section h2 {
      font-size: 1.8rem;
      margin-bottom: 10px;
      color: #444;
    }
    section h1 {
      font-size: 2rem;
      margin-bottom: 15px;
      color: #333;
    }
    p, li {
      margin-bottom: 10px;
    }
    ul {
      margin: 15px 0 30px 20px;
    }

    /* Buttons */
    .btn {
      display: inline-block;
      background: #007BFF;
      color: white;
      padding: 12px 20px;
      text-decoration: none;
      border-radius: 30px;
      font-weight: 600;
      transition: background 0.3s ease;
    }
    .btn:hover {
      background: #0056b3;
    }

    /* Footer */
    footer {
      text-align: center;
      margin-top: 40px;
      font-size: 14px;
      color: #aaa;
    }

    img {
      max-width: 100%;
      height: auto;
      margin-top: 20px;
      border-radius: 10px;
      box-shadow: 0px 2px 8px rgba(0,0,0,0.1);
    }

    /* Responsive */
    @media (max-width: 768px) {
      #main-content {
        margin-left: 0;
        padding: 20px;
      }
      #side-menu {
        width: 250px;
      }
    }
  </style>
</head>

<body>

  <!-- Menu Toggle Button -->
  <button id="menu-toggle">☰ Menu</button>

  <!-- Sidebar -->
  <nav id="side-menu">
    <a href="#introduction">1. Introduction</a>
    <a href="#rl-with-ppo-algorithm">2. Reinforcement Learning Solution</a>
    <a href="#mdp-process">3. MDP Framework</a>
    <a href="#how-rl-works">4. How RL Works</a>
    <a href="#rl-flowchart">5. RL Flowchart</a>
    <a href="#path-to-pricing">6. Pricing Path</a>
    <a href="#rl-stand-outs">7. RL Stand-Outs</a>
    <a href="#next-steps">8. Next Steps</a>
  </nav>

  <!-- Main Content -->
  <div id="main-content">
    <header>
      <h1>Dynamic Pricing Optimization using Reinforcement Learning</h1>
    </header>

    <section id="introduction">
      <h1>1. Introduction to Pricing Optimization with AI</h1>
      <p class="intro">
            In this <a href="https://drive.google.com/file/d/1HrD-uwzvoR0otjCDa0GFsPcZmI4YjSF6/view?usp=sharing" target="_blank">video presentation</a>, viewers will explore a dynamic pricing optimization model for personal insurance using reinforcement learning (RL), accessible via a previously <a href="https://actuarialangles-3.onrender.com" target="_blank", class="btn">shared dashboard</a>. The presentation addresses the pricing challenge of balancing accuracy, profitability, and customer value through a data-driven approach.
        </p>

        <h2>Key Points:</h2>

        <h3>Pricing Challenges</h3>
        <p>
            The model predicts insurance claim severities based on factors like age, vehicle use, and car model to price policies accurately and fairly. Pricing requires balancing three goals:
        </p>
        <ul>
            <li>Accurate severity predictions to avoid over- or underestimating claims.</li>
            <li>Competitive premiums to retain customers while covering costs.</li>
            <li>Customer satisfaction through fair pricing to build trust.</li>
        </ul>

        <h3>Limitations of Traditional Methods</h3>
        <p>
            Traditional approaches like Generalized Linear Models (GLM) and Bayesian optimization lack dynamic adaptability, struggling to adjust to new data or changing customer behaviors, resulting in less accurate and fair predictions.
        </p>

        <h3>Reinforcement Learning Solution</h3>
        <p>
            RL offers a dynamic, adaptive approach that fine-tunes model parameters (e.g., weights for age, vehicle use) through trial and error. Unlike traditional methods, RL learns from feedback, optimizing for accuracy, competitiveness, and fairness.
        </p>

        <h3>RL Results</h3>
        <p>
            Over 5120 optimization steps, RL reduced prediction errors, balanced over- and under-predictions, maintained a target loss ratio of 0.7, and outperformed Bayesian optimization with a lower root mean square error, ensuring more accurate and fair pricing.
        </p>

        <h3>What's Next?</h3>
        <p>
            The presentation will demonstrate how RL works, showcase achieved results, and discuss future improvements for better business and customer outcomes in insurance pricing. Stay tuned for an in-depth exploration of this innovative approach.
        </p>
      <p><a href="https://drive.google.com/file/d/1HrD-uwzvoR0otjCDa0GFsPcZmI4YjSF6/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <!-- Additional sections here -->
    <section id="rl-with-ppo-algorithm">
      <h1>2.My Solution to Reinforcement Learning</h1>
        <p class="intro">
            In this <a href="https://drive.google.com/file/d/19dGsdWYLUvWrgfrkUXI6NjqQouFVnLDO/view?usp=sharing" target="_blank">video presentation</a>, titled "My Solution to Reinforcement Learning," viewers will learn about an advanced solution for optimizing a model tuning dashboard for personal insurance pricing using reinforcement learning (RL) with the Proximal Policy Optimization (PPO) algorithm. The focus is on predicting insurance claim severity based on factors like age, vehicle use, and car model to set accurate and fair premiums.
        </p>

        <h3>Building the Solution with RL and PPO</h3>
        <ul>
            <li>RL, using the PPO algorithm, acts as a smart assistant that dynamically adjusts model weights (e.g., importance of age) and biases to fine-tune predictions.</li>
            <li>It balances multiple objectives: minimizing prediction errors, targeting a loss ratio of 0.7 for financial stability, and ensuring fairness by penalizing over- and under-predictions for low- and high-risk customers.</li>
            <li>RL outperforms static methods like Bayesian optimization, offering flexibility and adaptability.</li>
        </ul>

        <h3>Why RL Was Chosen</h3>
        <ul>
            <li>RL iteratively adapts to data patterns over time, unlike static methods that provide one-time solutions.</li>
            <li>Over 5120 steps, RL improved its reward from -660 to -267, ensuring the pricing model remains accurate and fair as customer behaviors or market conditions change.</li>
        </ul>

        <h3>Comparison with Bayesian Optimization</h3>
        <p>
            A bar chart compares RL to Bayesian optimization across key metrics:
			<img src="images/rl_vs_bayesian.png" alt="PowerPoint Presentation" 
           style="max-width: 100%; max-height: 500px; display: block; margin: 15px auto;">
        </p>
        <ul>
            <li><strong>Weighted Root Mean Square Error (RMSE):</strong> RL achieves a lower RMSE, indicating more accurate predictions closer to actual claim costs.</li>
            <li><strong>High-Risk Under-Prediction Penalty:</strong> RL prioritizes capturing true risk for high-risk customers, accepting higher penalties to avoid underestimation.</li>
            <li><strong>Low-Risk Over-Prediction Penalty:</strong> RL’s higher penalty reflects its focus on fairness, minimizing overcharging low-risk customers.</li>
            <li><strong>High-Risk Excessive Over-Prediction Penalty:</strong> RL’s adaptability captures complex data patterns, prioritizing overall accuracy.</li>
            <li><strong>Other Metrics (e.g., learning rate, bias):</strong> RL optimizes these marginally better than Bayesian.</li>
        </ul>
        <p>
            RL outperforms Bayesian across most metrics, delivering a more accurate, fair, and balanced pricing model.
        </p>

        <h3>Project Benefits</h3>
        <ul>
            <li><strong>Better Accuracy:</strong> Lower RMSE ensures premiums reflect true claim costs.</li>
            <li><strong>Improved Fairness:</strong> Balanced pricing for low- and high-risk customers builds trust and satisfaction.</li>
            <li><strong>Financial Balance:</strong> Maintains a 0.7 loss ratio for competitiveness and stability.</li>
            <li><strong>Adaptability:</strong> RL’s iterative learning keeps the model responsive to new data, keeping the business ahead.</li>
        </ul>

        <h3>Conclusion and Future Steps</h3>
        <p>
            The presentation concludes by highlighting RL with PPO as a superior, dynamic solution compared to traditional methods, with the bar chart demonstrating reduced errors and enhanced fairness. Future steps include refining RL, scaling to larger datasets, and applying it to new challenges. Viewers will see how RL’s adaptability and performance drive better accuracy, customer satisfaction, and financial outcomes for the insurance business.
        </p>
      <p><a href="https://drive.google.com/file/d/19dGsdWYLUvWrgfrkUXI6NjqQouFVnLDO/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="mdp-process">
      <h1>3. Markov Decision Process (MDP) Framework</h1>
        <p class="intro">
            In this <a href="https://drive.google.com/file/d/1IAGa4Cj8THogsUj-DB2QssGMLn646KbC/view?usp=sharing" target="_blank">video presentation</a>, titled "Markov Decision Process (MDP) Framework," viewers will explore how the MDP framework powers reinforcement learning (RL) in optimizing a model tuning dashboard for predicting insurance claim severities based on factors like age, vehicle use, and car model. The goal is to achieve accurate, fair, and financially balanced pricing.
        </p>

        <h2>MDP Framework Overview</h2>
        <p>
            The MDP framework, illustrated in a diagram, is the backbone of the RL approach, comprising four components in a cyclical process:
        </p>
        <ul>
            <li><strong>State:</strong> Represents the current model settings, including weights for factors (e.g., 0.5 for accuracy, 0.2 for age, 0.15 for vehicle use, 0.15 for car model) and a bias factor (e.g., 5.4).</li>
            <li><strong>Action:</strong> Small, deliberate tweaks to weights or bias (e.g., increasing age weight by 0.05 or decreasing bias by 0.01) to test configurations.</li>
            <li><strong>Reward:</strong> A score reflecting the action’s effectiveness, designed to minimize prediction errors, target a 0.7 loss ratio for financial balance, and ensure fairness by penalizing over-predictions for low-risk customers and under-predictions for high-risk customers.</li>
            <li><strong>Transition:</strong> Moves the agent to a new state with updated weights and bias after an action, restarting the cycle to refine the pricing model.</li>
        </ul>

        <h2>How MDP Fits the Project</h2>
        <ul>
            <li>The MDP framework is ideal as it enables systematic exploration of weight and bias combinations, learning from feedback without needing historical data (Markov property).</li>
            <li>Starting from an initial state, the RL agent makes tweaks, evaluates rewards, and transitions to new states, progressively optimizing the model for accuracy and fairness.</li>
			<img src="images/transition.png" alt="PowerPoint Presentation" 
           style="max-width: 100%; max-height: 300px; display: block; margin: 15px auto;">
        </ul>

        <h2>Results and Benefits</h2>
        <ul>
            <li>Over 5120 steps, the RL agent improved the reward from -660 to -267, indicating better predictions (to be shown in upcoming charts).</li>
            <li>The agent minimized root mean square error while maintaining a 0.7 loss ratio, ensuring fair and sustainable pricing.</li>
            <li>The structured MDP approach automates tedious manual tuning, saving time and enhancing outcomes for the insurance business.</li>
        </ul>

        <h2>Conclusion and Future Steps</h2>
        <p>
            The presentation emphasizes that the MDP framework provides a clear roadmap for RL to make data-driven adjustments, steadily improving model accuracy and fairness. Viewers will see how this structured approach optimizes pricing and learn about future refinements, such as enhancing the reward system or exploring additional parameters. Stay tuned for further insights and results.
        </p>
      <p><a href="https://drive.google.com/file/d/1IAGa4Cj8THogsUj-DB2QssGMLn646KbC/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="how-rl-works">
       <h1>4. How the Reinforcement Learning Process Works</h1>
        <p class="intro">
            In this <a href="https://drive.google.com/file/d/1ZVm2ubLcPbgun5feSWZlxpRLhjhqNy9g/view?usp=sharing" target="_blank">video presentation</a>, titled "How the Reinforcement Learning Process Works," viewers will gain insight into the mechanics of the reinforcement learning (RL) process powering the model tuning dashboard for predicting insurance claim severities based on factors like age, vehicle use, and car model. The aim is to set accurate and fair premiums.
        </p>

        <h3>Engine Under the Hood</h3>
        <p>
            The RL process comprises two core components:
        </p>
        <ul>
            <li><strong>Environment (Weight Optimization):</strong> Acts as a testing ground where the RL agent adjusts weights (e.g., importance of age, accuracy) and bias. The environment evaluates these tweaks by comparing predictions to actual claim costs, providing feedback via a reward (e.g., assessing if increasing age weight by 0.05 improves accuracy and fairness).</li>
            <li><strong>PPO Algorithm:</strong> The brain of the system, Proximal Policy Optimization (PPO) updates the agent’s strategy (policy) over ~5000 time steps. It ensures safe, gradual updates, refining the approach based on feedback. Over 5120 steps, the reward improved from -660 to -267, showing better adjustments (to be shown in a chart).</li>
        </ul>

        <h3>Convergence (Finding the Best Settings)</h3>
        <p>
            The RL process aims to converge on optimal weights and bias, minimizing the root mean square error (RMSE), which includes prediction errors, fairness across risk levels, and a target loss ratio of 0.7 for financial balance. By the end of 5120 steps, the agent achieved settings that significantly reduced errors, enhancing model accuracy and reliability.
        </p>

        <h3>Results (Outperforming Other Methods)</h3>
        <p>
            RL, using the weight optimization environment and PPO, outperformed Bayesian optimization by achieving a lower RMSE, indicating superior accuracy. It also ensured fairness by balancing predictions for low- and high-risk customers, crucial for equitable pricing.
        </p>

        <h3>Visualizing Progress</h3>
        <p>
            A chart (presented in the slide) illustrates the reward improvement from -670 to ~-200 over 5120 steps, with each point reflecting average performance across episodes. The curve rises, peaking around 4352 steps (highlighted in red), where the agent reaches optimal performance, minimizing errors and stabilizing the loss ratio. Beyond this, improvements plateau, signaling convergence.
			<img src="images/reward_curve.png" alt="PowerPoint Presentation" 
           style="max-width: 100%; max-height: 300px; display: block; margin: 15px auto;">
        </p>

        <h3>Conclusion and Future Steps</h3>
        <p>
            The presentation underscores that the RL process, driven by the weight optimization environment and PPO, optimizes the pricing model in a smart, automated way. It evaluates adjustments over 5000+ steps, converges to optimal settings, and surpasses methods like Bayesian optimization, delivering a more accurate, fair, and financially balanced model. Viewers will learn about the results and future steps to build on this success. Stay tuned for further details.
        </p>
      <p><a href="https://drive.google.com/file/d/1ZVm2ubLcPbgun5feSWZlxpRLhjhqNy9g/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="rl-flowchart">
      <h1>5. Step-by-Step RL Process Flowchart</h1>
      <h2>Reinforcement Learning Process Flowchart</h2>
        <p class="intro">
            In this <a href="https://drive.google.com/file/d/1_5p-Sbavv3WxriyIKTu1EFlaAq97a07h/view?usp=sharing" target="_blank">video presentation</a>, viewers will explore a detailed flowchart outlining the step-by-step reinforcement learning (RL) process for optimizing the model tuning dashboard, aimed at predicting insurance claim severities using factors like age, vehicle use, and car model to set accurate and fair premiums.
        </p>

        <h4>1. Launch RL System</h4>
        <p>
            Initialize the RL system with starting weights (e.g., 0.5 for accuracy, 0.2 for age, 0.15 for vehicle use, 0.15 for car model), a bias factor (e.g., 5.4), and the Proximal Policy Optimization (PPO) algorithm (configured with 128 steps, 64 batch size, 5 epochs, and a 0.0003 learning rate). This sets the initial pricing state.
        </p>

        <h4>2. Reset Environment</h4>
        <p>
            The weight optimization environment resets weights, bias, and risk-level (decile) adjustments to a clean starting point (e.g., initial weights or best state found), ensuring no interference from prior iterations.
        </p>

        <h4>3. Choose Adjustments</h4>
        <p>
            The RL agent selects tweaks to weights (range: -0.05 to 0.1), bias (range: -0.05 to 0.05), or risk-level adjustments (e.g., increasing age weight from 0.2 to 0.25 or decreasing bias from 5.4 to 5.39) to improve fairness, guided by PPO’s policy to maximize future rewards.
        </p>

        <h4>4. Update Pricing Model</h4>
        <p>
            Apply adjustments to the pricing model using XGBoost, a gradient boosting algorithm, to generate updated claim severity predictions. The model is rerun with new weights/bias, and predictions are evaluated against actual claim costs, transitioning to a new state.
        </p>

        <h4>5. Calculate Reward</h4>
        <p>
            The environment computes a reward based on:
        </p>
        <ul>
            <li><strong>Root Mean Square Error (RMSE):</strong> Measures prediction accuracy (lower is better).</li>
            <li><strong>Loss Ratio:</strong> Targets 0.7 (claims paid/premiums collected), with penalties for deviations.</li>
            <li><strong>Fairness Penalties:</strong> For over-predictions (low-risk customers), under-predictions (high-risk customers), and excessive over-predictions (high-risk).</li>
        </ul>
        <p>
            The reward function (to be shown in Python) combines RMSE, loss ratio penalty, fairness penalties, and bias regularization. Rewards improved from -660 to -267 over 5120 steps, indicating better adjustments.
			
        </p>

        <h4>6. Train PPO Agent</h4>
        <p>
            The PPO agent uses reward feedback to refine its policy over 5 epochs and 64 batches, collecting state, action, and reward data. Techniques like clipped surrogate objectives ensure stable updates, favoring high-reward adjustments (e.g., increasing age weight) in future cycles.
        </p>

        <h4>7. Track Best Settings</h4>
        <p>
            The system monitors RMSE improvements. If new settings lower RMSE (e.g., from 100 to 90), they are saved as the best state. Optimal settings were found at ~4352 steps with a reward of -267, where improvements plateaued.
        </p>

        <h4>8. Decision Point</h4>
        <p>
            Check if training has reached 5120 steps (40 iterations of 128 steps each). If not, loop back to step 2 (reset environment) to continue training. If yes, proceed to step 9.
        </p>

        <h4>9. End Process</h4>
        <p>
            Training concludes, and the optimized pricing model (using the best settings from step 7) is ready for analysis/deployment. The final model at 4352 steps, with a reward of -267, achieved lower RMSE, maintained a 0.7 loss ratio, ensured fairness, and slightly outperformed Bayesian optimization.
        </p>

        <h3>Conclusion</h3>
        <p>
            The flowchart illustrates the iterative process—state, action, transition, and decision—systematically optimizing the pricing model. It improved rewards from -660 to -267, reduced prediction errors, and balanced accuracy, financial stability, and customer satisfaction, making the model robust and ready for deployment. Viewers will understand the comprehensive workflow from initialization to finalization, with clear visuals of the process.
			<img src="images/Flow_Chart.png" 
                 alt="PowerPoint Presentation" 
                 style="width: 50%; height: auto; max-height: 500px; display: block; margin: 20px auto; image-rendering: crisp-edges;">
        </p>
      <p><a href="https://drive.google.com/file/d/1_5p-Sbavv3WxriyIKTu1EFlaAq97a07h/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="path-to-pricing">
      <h2>6. Path to Pricing Grid</h2>
      <p class="intro">
            In this video presentation, viewers will explore a visual representation of the reinforcement learning (RL) journey for optimizing the model tuning dashboard, aimed at predicting insurance claim severities using factors like age, vehicle use, and car model to set accurate and fair premiums.
        </p>

        <h2>Understanding the State-Action Grid</h2>
        <p>
            A diagram illustrates a grid where each square represents a state (model settings, e.g., weights of 0.5 for accuracy, 0.2 for age, 0.15 for vehicle use, 0.15 for car model, and a 5.4 bias factor).
        </p>
        <ul>
            <li>Actions are small tweaks (e.g., increasing age weight by 0.05 or decreasing bias by 0.01) to improve the model.</li>
            <li>Rewards (shown as brown buttons) provide feedback, scoring the tweak’s impact on accuracy, fairness across risk levels, and a target 0.7 loss ratio for financial stability. Higher rewards indicate better performance.</li>
        </ul>

        <h2>Agent’s Journey Through the Grid</h2>
        <p>
            The RL agent starts at the bottom-left state (initial settings) and follows arrows to new states by taking actions (e.g., adjusting age weight).
        </p>
        <ul>
            <li>The environment provides rewards based on the new state’s performance (e.g., high reward for reducing errors without overcharging low-risk customers, low reward for under-predicting high-risk claims).</li>
            <li>The agent explores, loops back, or tries new directions, learning iteratively to approach the goal state (bottom-right, green) with optimal weights and bias for accurate, fair, and sustainable pricing. Over 5120 steps, rewards improved from -660 to -267, with peak performance at 4352 steps (to be shown in charts).</li>
        </ul>

        <h2>Why This Process Matters</h2>
        <p>
            The state-action-reward cycle makes RL effective, systematically exploring settings, learning from feedback, and converging on an optimal solution, unlike traditional methods.
        </p>
        <ul>
            <li><strong>Accuracy:</strong> Reduced prediction errors for reliable claim severity predictions.</li>
            <li><strong>Fairness:</strong> Balanced pricing to avoid overcharging low-risk or undercharging high-risk customers.</li>
            <li><strong>Efficiency:</strong> Automated process saves time compared to manual trial-and-error, delivering faster, better results.</li>
        </ul>

        <h2>Conclusion</h2>
        <p>
            The presentation concludes that the state-action grid visualizes how the RL agent navigates states, actions, and rewards to achieve a more accurate pricing model. The process has delivered impressive results, with plans to enhance the reward function with customer-focused metrics for an even better model. Viewers will gain a clear understanding of the RL journey through the grid diagram and its impact on pricing optimization. Stay tuned for further insights.
			<img src="images/path_grid.png" 
                 alt="PowerPoint Presentation" 
                 style="width: 50%; height: auto; max-height: 300px; display: block; margin: 15px auto;">
        </p>
      <p><a href="https://drive.google.com/file/d/1rRIW5LMTy89Av-DhHBkMRhfCdqjqHNae/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="rl-stand-outs">
      <h2>7. RL Stands out</h2>
      <p class="intro">
            In this video presentation, viewers will explore why reinforcement learning (RL) outperforms traditional optimization methods for fine-tuning the model tuning dashboard, which predicts insurance claim severities based on factors like age, vehicle use, and car model to set accurate and fair premiums.
        </p>

        <h2>Comparing RL to Traditional Methods</h2>
        <ul>
            <li><strong>Bayesian Optimization:</strong> Systematic but less adaptive, exploring a fixed range of options. It struggles with complex weight and bias combinations, resulting in a higher root mean square error (RMSE) compared to RL due to its inability to adjust to new data patterns or changing conditions.</li>
            <li><strong>Random Search:</strong> Lacks guidance, trying random combinations inefficiently. It takes longer to find decent settings and performs worse than Bayesian optimization, failing to match RL’s effectiveness.</li>
            <li><strong>Reinforcement Learning:</strong> Dynamically balances multiple goals (minimizing prediction errors, ensuring fairness, targeting a 0.7 loss ratio) through trial and error. Over 5120 steps, RL improved rewards from -660 to -267, refining the model for better results (shown in a progression graph).</li>
        </ul>

        <h2>RL’s Adaptability Unlocks Better Insights</h2>
        <p>
            RL’s key advantage is its adaptability, exploiting data patterns missed by Bayesian and random search’s rigid or random strategies. It identifies optimal weights and bias, reducing RMSE while ensuring fairness (e.g., avoiding overcharging low-risk customers or undercharging high-risk ones). RL continuously improves with new data, keeping the pricing model accurate and relevant in dynamic insurance markets.
        </p>

        <h2>Results Speak for Themselves</h2>
        <p>
            RL outperformed Bayesian optimization and random search, achieving a lower RMSE, balancing over- and under-predictions across risk levels, and maintaining a 0.7 loss ratio. A previously shared chart highlights RL’s superior performance in metrics like RMSE and fairness penalties, confirming its dynamic approach delivers a more accurate and fair pricing model.
        </p>

        <h2>Conclusion</h2>
        <p>
            The presentation concludes that RL stands out for its dynamic goal balancing and adaptability, outperforming traditional methods like Bayesian optimization and random search. It has enabled a more accurate, fair, and financially balanced pricing model, with the potential to tackle new challenges or scale to larger datasets. Viewers are encouraged to explore the model tuning dashboard to see RL’s accuracy in action, noting that while it takes time, it delivers precise results aligned with strategic goals.
        </p>
      <p><a href="https://drive.google.com/file/d/1wVITpugxcmPcWNt2RKE3zGx2vtpJiDf2/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <section id="next-steps">
      <h2>8. Next Steps - Customer Perceived Value</h2>
      <p class="intro">
            In this video presentation, viewers will learn about the future enhancements planned for the reinforcement learning (RL) system used in the model tuning dashboard, which predicts insurance claim severities based on factors like age, vehicle use, and car model. The focus is on integrating customer-focused metrics to enhance accuracy, fairness, and customer satisfaction in pricing.
        </p>

        <h2>Feature Enhancement: Adding Customer Metrics</h2>
        <p>
            The RL system will be upgraded by incorporating customer perceived value metrics into the reward function to balance accuracy with customer satisfaction. These metrics include:
        </p>
        <ul>
            <li><strong>Retention Rate:</strong> Encourages premiums that promote policy renewals by avoiding overcharging low-risk customers, fostering trust and loyalty.</li>
            <li><strong>Conversion Rate:</strong> Boosts new sales by setting competitive, fair premiums to attract new customers.</li>
            <li><strong>Churn Rate:</strong> Minimizes customer losses by penalizing pricing strategies that lead to high churn, such as overpredictions causing unfairly high premiums.</li>
        </ul>

        <h2>Current vs. Future Reward Metrics</h2>
        <p>
            A pie chart compares the current reward function (25% root mean square error for accuracy, 25% loss ratio targeting 0.7 for financial stability, 50% fairness penalties) with the future one.
        </p>
        <p>
            The future reward function will allocate 50% to customer metrics (20% retention rate, 15% conversion rate, 15% churn rate) and 50% to accuracy (25%) and loss ratio (25%), prioritizing customer satisfaction alongside existing goals.
        </p>
		<img src="images/customer_perspective.png" 
                 alt="PowerPoint Presentation" 
                 style="width: 30%; height: auto; max-height: 300px; display: block; margin: 15px auto;">

        <h2>How to Make It Happen</h2>
        <p>
            The reward function in the weight optimization environment will be updated to include retention, conversion, and churn rate terms. For example, the RL agent will be rewarded for premiums that drive renewals or penalized for pricing that increases churn.
        </p>
        <p>
            The agent will learn to balance these new metrics with RMSE and loss ratio, dynamically adjusting weights and bias to optimize for accuracy, financial stability, and customer satisfaction.
        </p>

        <h2>Impact on the Book of Business</h2>
        <ul>
            <li><strong>Happier Customers:</strong> Fair, competitive pricing will increase renewals, attract new customers, and reduce churn.</li>
            <li><strong>Balanced Outcomes:</strong> Maintains accuracy (via RMSE) and financial stability (via loss ratio) while prioritizing customer needs.</li>
            <li><strong>Competitive Edge:</strong> A customer-centric pricing model will differentiate the insurer in the market, enhancing customer retention and acquisition while ensuring financial health.</li>
        </ul>

        <h2>Conclusion</h2>
        <p>
            The presentation concludes that integrating retention rate, conversion rate, and churn rate into the RL reward function will create a customer-first pricing model, balancing accuracy, financial stability, and satisfaction. This will lead to better business outcomes and a stronger market position. Viewers are invited to share feedback via email and stay tuned for updates on the enhanced project, expected in two weeks.
        </p>
      <p><a href="https://drive.google.com/file/d/15e-_qIMvulHKJMG-rqsPUapMwtHOG48R/view?usp=sharing" target="_blank" class="btn">Watch the Video</a></p>
    </section>

    <footer>
      <p><a href="#top" class="btn">Back to Top</a></p>
    </footer>
	<footer>
      <p><a href="index.html" class="btn">Back to Home</a></p>
    </footer>
  </div>

  <!-- Toggle Sidebar Script -->
  <script>
    const menuToggle = document.getElementById('menu-toggle');
    const sideMenu = document.getElementById('side-menu');
    const menuLinks = document.querySelectorAll('#side-menu a');

    menuToggle.addEventListener('click', () => {
      sideMenu.classList.toggle('open');
    });

    menuLinks.forEach(link => {
      link.addEventListener('click', () => {
        sideMenu.classList.remove('open');
      });
    });
  </script>
</body>
</html>
